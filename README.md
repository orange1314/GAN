{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb99e5d-4766-462c-8e77-0ed33a251646",
   "metadata": {},
   "source": [
    "GAN 的核心概念是由兩個神經網絡，生成器（Generator）和鑑別器（Discriminator），透過對抗過程進行訓練。以下是這個過程的數學描述：\n",
    "\n",
    "### 基本組件\n",
    "\n",
    "1. **生成器 (G)**: 試圖創造出看起來像真實數據的數據。它接受一個隨機噪聲向量 $z$（從某個潛在空間分布中抽樣得到），並生成數據 $G(z)$。\n",
    "\n",
    "2. **鑑別器 (D)**: 試圖區分真實數據和生成器創造的假數據。對於給定的輸入數據 $x$，鑑別器輸出一個表示數據為真實數據的概率 $D(x)$。\n",
    "\n",
    "### 目標函數\n",
    "\n",
    "GAN 的訓練過程可以看作是一個迷你-最大博弈遊戲（minimax game），其中生成器和鑑別器各自嘗試最小化和最大化以下目標函數：\n",
    "\n",
    "$\\min_{G} \\max_{D} V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1 - D(G(z)))] $\n",
    "\n",
    "這裡：\n",
    "- $ \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] $ 是鑑別器評估真實數據的期望對數概率。鑑別器希望\n",
    "\n",
    "這部分的值最大化，意味著對於真實數據 $x$，鑑別器 $D(x)$ 應該接近 1（即判斷為真實的概率高）。\n",
    "\n",
    "- $ \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1 - D(G(z)))] $ 是鑑別器評估由生成器創造的假數據的期望對數概率。在這裡，生成器 $G$ 產生數據 $G(z)$，而鑑別器 $D$ 試圖將其判斷為假（即 $D(G(z))$ 接近 0）。生成器的目標是讓這部分的值最小化，意味著生成的數據足夠好以至於鑑別器將它們誤判為真實數據。\n",
    "\n",
    "### 訓練過程\n",
    "\n",
    "在實際訓練過程中，我們會交替進行以下步驟：\n",
    "\n",
    "1. **訓練鑑別器 (D)**: 固定生成器 $G$，更新鑑別器 $D$ 以最大化 $ \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1 - D(G(z)))] $。這通常通過梯度上升法實現。\n",
    "\n",
    "2. **訓練生成器 (G)**: 固定鑑別器 $D$，更新生成器 $G$ 以最小化 $ \\mathbb{E}_{z \\sim p_{z}(z)}[\\log (1 - D(G(z)))] $。這通常通過梯度下降法實現。\n",
    "\n",
    "### 收斂\n",
    "\n",
    "理想的情況是，隨著訓練的進行，生成器會越來越擅長創造看起來真實的數據，而鑑別器則會越來越擅長區分真假數據，直到達到一個均衡點。在這個點上，鑑別器不能再準確地區分真假數據，即對於所有 $x$，$D(x) = 0.5$。\n",
    "\n",
    "總的來說，GAN 透過一個對抗性的訓練過程，使得生成器能夠產生越來越逼真的數據，而鑑別器則試圖以最佳的策略來\n",
    "\n",
    "區分真假數據。這個過程促使兩者不斷進化和改進，最終達到一種動態平衡，這種平衡狀態下的生成器能夠產生非常接近真實數據的輸出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0b084-8652-410a-a2b7-d3fd5a3ce623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten, Reshape, LeakyReLU, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv2DTranspose, BatchNormalization\n",
    "from keras.layers import Conv2D, Dropout, Flatten\n",
    "\n",
    "# 設定\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100\n",
    "\n",
    "# 建立生成器\n",
    "\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "\n",
    "    # 將輸入映射到一個 7x7x128 的特徵圖\n",
    "    model.add(Dense(7 * 7 * 128, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 上採樣到 14x14\n",
    "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # 上採樣到 28x28 (MNIST 圖像大小)\n",
    "    model.add(Conv2DTranspose(64, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2DTranspose(1, kernel_size=3, strides=1, padding='same', activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# 編譯鑑別器\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(0.0002, 0.5),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# 編譯生成器\n",
    "generator = build_generator()\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "# 唯有當生成器和鑑別器合併時，才訓練生成器\n",
    "discriminator.trainable = False\n",
    "valid = discriminator(img)\n",
    "\n",
    "# 合併生成器和鑑別器\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# 訓練\n",
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "    # 載入 MNIST 數據集\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # 正規化\n",
    "    X_train = X_train / 127.5 - 1.\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  訓練鑑別器\n",
    "        # ---------------------\n",
    "\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  訓練生成器\n",
    "        # ---------------------\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "        # 打印進度\n",
    "        print (\"%d [鑑別器損失: %f, 準確率.: %.2f%%] [生成器損失: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "        # 如果到了保存的間隔，就保存生成的圖像樣本\n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "# 保存圖像\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # 正規化圖像\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"mnist_%d.png\" % epoch)\n",
    "    plt.close()\n",
    "\n",
    "# 訓練生成器和鑑別器\n",
    "train(epochs=200000, batch_size=32, save_interval=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a5f38-ffcb-4f38-8bc6-8fd395f2364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "\n",
    "def create_gif(input_folder, output_file, duration):\n",
    "    images = []\n",
    "    for file_name in sorted(os.listdir(input_folder)):\n",
    "        if file_name.endswith('.png'):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            images.append(imageio.imread(file_path))\n",
    "    imageio.mimsave(output_file, images, duration=duration)\n",
    "\n",
    "# 使用示例\n",
    "input_folder = r'C:\\Users\\B20770\\Desktop\\Lin\\深度學習\\GAN'  # 將此路徑替換為您圖像的存儲位置\n",
    "output_file = 'output.gif'  # GIF的輸出文件名\n",
    "duration = 0.5  # 每幀之間的持續時間，以秒為單位\n",
    "\n",
    "create_gif(input_folder, output_file, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b47d2f-d124-47e3-82d6-1e12cdbb68b5",
   "metadata": {},
   "source": [
    "![](output.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8d959-c968-4240-8b0d-8f5ead44662a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
